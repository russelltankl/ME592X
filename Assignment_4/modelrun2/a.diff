diff --git a/LICENSE b/LICENSE
old mode 100644
new mode 100755
diff --git a/README.md b/README.md
old mode 100644
new mode 100755
diff --git a/cnn.py b/cnn.py
old mode 100644
new mode 100755
index 06d092b..98f38cd
--- a/cnn.py
+++ b/cnn.py
@@ -30,6 +30,7 @@ def getModel(img_width, img_height, img_channels, output_dim, weights_path):
        model: A Model instance.
     """
     model = cnn_models.resnet8(img_width, img_height, img_channels, output_dim)
+    
 
     if weights_path:
         try:
@@ -63,7 +64,7 @@ def trainModel(train_data_generator, val_data_generator, model, initial_epoch):
     # Configure training process
     model.compile(loss=[utils.hard_mining_mse(model.k_mse),
                         utils.hard_mining_entropy(model.k_entropy)],
-                        optimizer='adam', decay=1e-5, loss_weights=[model.alpha, model.beta])
+                        optimizer='adam', loss_weights=[model.alpha, model.beta])
 
     # Save model with the lowest validation loss
     weights_path = os.path.join(FLAGS.experiment_rootdir, 'weights_{epoch:03d}.h5')
@@ -90,75 +91,86 @@ def trainModel(train_data_generator, val_data_generator, model, initial_epoch):
 
 
 def _main():
+    #with tf.device('/device:GPU:0'):
+        #print(os.environ['CUDA_VISIBLE_DEVICES']="0")## assign which device to use (allow 1)
+        os.environ['CUDA_VISIBLE_DEVICES']="0"
+        # session = tf.Session(config=tf.ConfigProto(log_device_placement=True))
+       
+        from keras import backend as K
+        config = tf.ConfigProto()
+        sess = tf.Session(config=config)
+        config.gpu_options.allow_growth = True
+        
+        K.set_session(sess)
 
-    # Create the experiment rootdir if not already there
-    if not os.path.exists(FLAGS.experiment_rootdir):
-        os.makedirs(FLAGS.experiment_rootdir)
+        # Create the experiment rootdir if not already there
+        if not os.path.exists(FLAGS.experiment_rootdir):
+            os.makedirs(FLAGS.experiment_rootdir)
 
-    # Input image dimensions
-    img_width, img_height = FLAGS.img_width, FLAGS.img_height
-    
-    # Cropped image dimensions
-    crop_img_width, crop_img_height = FLAGS.crop_img_width, FLAGS.crop_img_height
-
-    # Image mode
-    if FLAGS.img_mode=='rgb':
-        img_channels = 3
-    elif FLAGS.img_mode == 'grayscale':
-        img_channels = 1
-    else:
-        raise IOError("Unidentified image mode: use 'grayscale' or 'rgb'")
+        # Input image dimensions
+        img_width, img_height = FLAGS.img_width, FLAGS.img_height
         
-    # Output dimension (one for steering and one for collision)
-    output_dim = 1
-
-    # Generate training data with real-time augmentation
-    train_datagen = utils.DroneDataGenerator(rotation_range = 0.2,
-                                             rescale = 1./255,
-                                             width_shift_range = 0.2,
-                                             height_shift_range=0.2)
-
-    train_generator = train_datagen.flow_from_directory(FLAGS.train_dir,
-                                                        shuffle = True,
-                                                        color_mode=FLAGS.img_mode,
-                                                        target_size=(img_width, img_height),
-                                                        crop_size=(crop_img_height, crop_img_width),
-                                                        batch_size = FLAGS.batch_size)
-
-    # Generate validation data with real-time augmentation
-    val_datagen = utils.DroneDataGenerator(rescale = 1./255)
-
-    val_generator = val_datagen.flow_from_directory(FLAGS.val_dir,
-                                                        shuffle = True,
-                                                        color_mode=FLAGS.img_mode,
-                                                        target_size=(img_width, img_height),
-                                                        crop_size=(crop_img_height, crop_img_width),
-                                                        batch_size = FLAGS.batch_size)
-
-    # Weights to restore
-    weights_path = os.path.join(FLAGS.experiment_rootdir, FLAGS.weights_fname)
-    initial_epoch = 0
-    if not FLAGS.restore_model:
-        # In this case weights will start from random
-        weights_path = None
-    else:
-        # In this case weigths will start from the specified model
-        initial_epoch = FLAGS.initial_epoch
-
-    # Define model
-    model = getModel(crop_img_width, crop_img_height, img_channels,
-                        output_dim, weights_path)
-
-    # Save the architecture of the model as png
-    plot_arch_path = os.path.join(FLAGS.experiment_rootdir, 'architecture.png')
-    plot_model(model, to_file=plot_arch_path)
-
-    # Serialize model into json
-    json_model_path = os.path.join(FLAGS.experiment_rootdir, FLAGS.json_model_fname)
-    utils.modelToJson(model, json_model_path)
-
-    # Train model
-    trainModel(train_generator, val_generator, model, initial_epoch)
+        # Cropped image dimensions
+        crop_img_width, crop_img_height = FLAGS.crop_img_width, FLAGS.crop_img_height
+
+        # Image mode
+        if FLAGS.img_mode=='rgb':
+            img_channels = 3
+        elif FLAGS.img_mode == 'grayscale':
+            img_channels = 1
+        else:
+            raise IOError("Unidentified image mode: use 'grayscale' or 'rgb'")
+            
+        # Output dimension (one for steering and one for collision)
+        output_dim = 1
+
+        # Generate training data with real-time augmentation
+        train_datagen = utils.DroneDataGenerator(rotation_range = 0.2,
+                                                 rescale = 1./255,
+                                                 width_shift_range = 0.2,
+                                                 height_shift_range=0.2)
+
+        train_generator = train_datagen.flow_from_directory(FLAGS.train_dir,
+                                                            shuffle = True,
+                                                            color_mode=FLAGS.img_mode,
+                                                            target_size=(img_width, img_height),
+                                                            crop_size=(crop_img_height, crop_img_width),
+                                                            batch_size = FLAGS.batch_size)
+
+        # Generate validation data with real-time augmentation
+        val_datagen = utils.DroneDataGenerator(rescale = 1./255)
+
+        val_generator = val_datagen.flow_from_directory(FLAGS.val_dir,
+                                                            shuffle = True,
+                                                            color_mode=FLAGS.img_mode,
+                                                            target_size=(img_width, img_height),
+                                                            crop_size=(crop_img_height, crop_img_width),
+                                                            batch_size = FLAGS.batch_size)
+
+        # Weights to restore
+        weights_path = os.path.join(FLAGS.experiment_rootdir, FLAGS.weights_fname)
+        initial_epoch = 0
+        if not FLAGS.restore_model:
+            # In this case weights will start from random
+            weights_path = None
+        else:
+            # In this case weigths will start from the specified model
+            initial_epoch = FLAGS.initial_epoch
+
+        # Define model
+        model = getModel(crop_img_width, crop_img_height, img_channels,
+                            output_dim, weights_path)
+
+        # Save the architecture of the model as png
+       # plot_arch_path = os.path.join(FLAGS.experiment_rootdir, 'architecture.dot')
+       # plot_model(model, to_file=plot_arch_path)
+
+        # Serialize model into json
+        json_model_path = os.path.join(FLAGS.experiment_rootdir, FLAGS.json_model_fname)
+        utils.modelToJson(model, json_model_path)
+
+        # Train model
+        trainModel(train_generator, val_generator, model, initial_epoch)
 
 
 def main(argv):
diff --git a/cnn_models.py b/cnn_models.py
old mode 100644
new mode 100755
index d6ea889..f8e3a9b
--- a/cnn_models.py
+++ b/cnn_models.py
@@ -1,9 +1,218 @@
+from __future__ import print_function
+from __future__ import absolute_import
+from __future__ import division
+
+
 import keras
 from keras.models import Model
 from keras.layers import Dense, Dropout, Activation, Flatten, Input
 from keras.layers import Conv2D, MaxPooling2D
 from keras.layers.merge import add
 from keras import regularizers
+from keras import initializers
+from keras import constraints
+from keras.engine import Layer
+from keras.engine import InputSpec
+from keras.utils import conv_utils
+from keras.legacy import interfaces
+#import depth_class
+from keras.layers import DepthwiseConv2D
+#import DepthwiseConv2D
+#from keras.applications import mobilenet
+from keras import backend as K
+
+# class DepthwiseConv2D(Conv2D):
+#     """Depthwise separable 2D convolution.
+#     Depthwise Separable convolutions consists in performing
+#     just the first step in a depthwise spatial convolution
+#     (which acts on each input channel separately).
+#     The `depth_multiplier` argument controls how many
+#     output channels are generated per input channel in the depthwise step.
+#     # Arguments
+#         kernel_size: An integer or tuple/list of 2 integers, specifying the
+#             width and height of the 2D convolution window.
+#             Can be a single integer to specify the same value for
+#             all spatial dimensions.
+#         strides: An integer or tuple/list of 2 integers,
+#             specifying the strides of the convolution along the width and height.
+#             Can be a single integer to specify the same value for
+#             all spatial dimensions.
+#             Specifying any stride value != 1 is incompatible with specifying
+#             any `dilation_rate` value != 1.
+#         padding: one of `'valid'` or `'same'` (case-insensitive).
+#         depth_multiplier: The number of depthwise convolution output channels
+#             for each input channel.
+#             The total number of depthwise convolution output
+#             channels will be equal to `filters_in * depth_multiplier`.
+#         data_format: A string,
+#             one of `channels_last` (default) or `channels_first`.
+#             The ordering of the dimensions in the inputs.
+#             `channels_last` corresponds to inputs with shape
+#             `(batch, height, width, channels)` while `channels_first`
+#             corresponds to inputs with shape
+#             `(batch, channels, height, width)`.
+#             It defaults to the `image_data_format` value found in your
+#             Keras config file at `~/.keras/keras.json`.
+#             If you never set it, then it will be 'channels_last'.
+#         activation: Activation function to use
+#             (see [activations](../activations.md)).
+#             If you don't specify anything, no activation is applied
+#             (ie. 'linear' activation: `a(x) = x`).
+#         use_bias: Boolean, whether the layer uses a bias vector.
+#         depthwise_initializer: Initializer for the depthwise kernel matrix
+#             (see [initializers](../initializers.md)).
+#         bias_initializer: Initializer for the bias vector
+#             (see [initializers](../initializers.md)).
+#         depthwise_regularizer: Regularizer function applied to
+#             the depthwise kernel matrix
+#             (see [regularizer](../regularizers.md)).
+#         bias_regularizer: Regularizer function applied to the bias vector
+#             (see [regularizer](../regularizers.md)).
+#         activity_regularizer: Regularizer function applied to
+#             the output of the layer (its 'activation').
+#             (see [regularizer](../regularizers.md)).
+#         depthwise_constraint: Constraint function applied to
+#             the depthwise kernel matrix
+#             (see [constraints](../constraints.md)).
+#         bias_constraint: Constraint function applied to the bias vector
+#             (see [constraints](../constraints.md)).
+#     # Input shape
+#         4D tensor with shape:
+#         `[batch, channels, rows, cols]` if data_format='channels_first'
+#         or 4D tensor with shape:
+#         `[batch, rows, cols, channels]` if data_format='channels_last'.
+#     # Output shape
+#         4D tensor with shape:
+#         `[batch, filters, new_rows, new_cols]` if data_format='channels_first'
+#         or 4D tensor with shape:
+#         `[batch, new_rows, new_cols, filters]` if data_format='channels_last'.
+#         `rows` and `cols` values might have changed due to padding.
+#     """
+
+#     def __init__(self,
+#                  kernel_size,
+#                  strides=(1, 1),
+#                  padding='valid',
+#                  depth_multiplier=1,
+#                  data_format=None,
+#                  activation=None,
+#                  use_bias=True,
+#                  depthwise_initializer='glorot_uniform',
+#                  bias_initializer='zeros',
+#                  depthwise_regularizer=None,
+#                  bias_regularizer=None,
+#                  activity_regularizer=None,
+#                  depthwise_constraint=None,
+#                  bias_constraint=None,
+#                  **kwargs):
+#         super(DepthwiseConv2D, self).__init__(
+#             filters=None,
+#             kernel_size=kernel_size,
+#             strides=strides,
+#             padding=padding,
+#             data_format=data_format,
+#             activation=activation,
+#             use_bias=use_bias,
+#             bias_regularizer=bias_regularizer,
+#             activity_regularizer=activity_regularizer,
+#             bias_constraint=bias_constraint,
+#             **kwargs)
+#         self.depth_multiplier = depth_multiplier
+#         self.depthwise_initializer = initializers.get(depthwise_initializer)
+#         self.depthwise_regularizer = regularizers.get(depthwise_regularizer)
+#         self.depthwise_constraint = constraints.get(depthwise_constraint)
+#         self.bias_initializer = initializers.get(bias_initializer)
+
+#     def build(self, input_shape):
+#         if len(input_shape) < 4:
+#             raise ValueError('Inputs to `DepthwiseConv2D` should have rank 4. '
+#                              'Received input shape:', str(input_shape))
+#         if self.data_format == 'channels_first':
+#             channel_axis = 1
+#         else:
+#             channel_axis = 3
+#         if input_shape[channel_axis] is None:
+#             raise ValueError('The channel dimension of the inputs to '
+#                              '`DepthwiseConv2D` '
+#                              'should be defined. Found `None`.')
+#         input_dim = int(input_shape[channel_axis])
+#         depthwise_kernel_shape = (self.kernel_size[0],
+#                                   self.kernel_size[1],
+#                                   input_dim,
+#                                   self.depth_multiplier)
+
+#         self.depthwise_kernel = self.add_weight(
+#             shape=depthwise_kernel_shape,
+#             initializer=self.depthwise_initializer,
+#             name='depthwise_kernel',
+#             regularizer=self.depthwise_regularizer,
+#             constraint=self.depthwise_constraint)
+
+#         if self.use_bias:
+#             self.bias = self.add_weight(shape=(input_dim * self.depth_multiplier,),
+#                                         initializer=self.bias_initializer,
+#                                         name='bias',
+#                                         regularizer=self.bias_regularizer,
+#                                         constraint=self.bias_constraint)
+#         else:
+#             self.bias = None
+#         # Set input spec.
+#         self.input_spec = InputSpec(ndim=4, axes={channel_axis: input_dim})
+#         self.built = True
+
+#     def call(self, inputs, training=None):
+#         outputs = K.depthwise_conv2d(
+#             inputs,
+#             self.depthwise_kernel,
+#             strides=self.strides,
+#             padding=self.padding,
+#             dilation_rate=self.dilation_rate,
+#             data_format=self.data_format)
+
+#         if self.bias:
+#             outputs = K.bias_add(
+#                 outputs,
+#                 self.bias,
+#                 data_format=self.data_format)
+
+#         if self.activation is not None:
+#             return self.activation(outputs)
+
+#         return outputs
+
+#     def compute_output_shape(self, input_shape):
+#         if self.data_format == 'channels_first':
+#             rows = input_shape[2]
+#             cols = input_shape[3]
+#             out_filters = input_shape[1] * self.depth_multiplier
+#         elif self.data_format == 'channels_last':
+#             rows = input_shape[1]
+#             cols = input_shape[2]
+#             out_filters = input_shape[3] * self.depth_multiplier
+
+#         rows = conv_utils.conv_output_length(rows, self.kernel_size[0],
+#                                              self.padding,
+#                                              self.strides[0])
+#         cols = conv_utils.conv_output_length(cols, self.kernel_size[1],
+#                                              self.padding,
+#                                              self.strides[1])
+#         if self.data_format == 'channels_first':
+#             return (input_shape[0], out_filters, rows, cols)
+#         elif self.data_format == 'channels_last':
+#             return (input_shape[0], rows, cols, out_filters)
+
+#     def get_config(self):
+#         config = super(DepthwiseConv2D, self).get_config()
+#         config.pop('filters')
+#         config.pop('kernel_initializer')
+#         config.pop('kernel_regularizer')
+#         config.pop('kernel_constraint')
+#         config['depth_multiplier'] = self.depth_multiplier
+#         config['depthwise_initializer'] = initializers.serialize(self.depthwise_initializer)
+#         config['depthwise_regularizer'] = regularizers.serialize(self.depthwise_regularizer)
+#         config['depthwise_constraint'] = constraints.serialize(self.depthwise_constraint)
+#         return config
+
 
 
 
@@ -91,3 +300,72 @@ def resnet8(img_width, img_height, img_channels, output_dim):
     print(model.summary())
 
     return model
+'''
+def MobileNet(input_shape=None,
+              alpha=1.0,
+              depth_multiplier=1,
+              dropout=1e-3,
+              include_top=True,
+              weights='imagenet',
+              input_tensor=None,
+              pooling=None,
+              classes=1000,
+              output_dim):
+
+'''
+
+def MobileNet(img_width, img_height, img_channels, output_dim,
+              depth_multiplier=1,input_tensor=None,
+              dropout=1e-3):
+    """
+    Define model architecture.
+    
+    # Arguments
+       img_width: Target image widht.
+       img_height: Target image height.
+       img_channels: Target image channels.
+       output_dim: Dimension of model output.
+       
+    # Returns
+       model: A Model instance.
+    """
+
+    # Input
+    img_input = Input(shape=(img_height, img_width, img_channels))
+
+
+    x1 = Conv2D(32, (5, 5), strides=[2,2], padding='same')(img_input)
+    x1 = MaxPooling2D(pool_size=(3, 3), strides=[2,2])(x1)
+
+    # First residual block
+    x2=DepthwiseConv2D((3, 3),
+                        padding='valid',
+                        depth_multiplier=depth_multiplier,
+                        strides=(2,2),
+                        use_bias=False)(x1)
+    x2 = keras.layers.normalization.BatchNormalization()(x2)
+    x2 = Activation('relu')(x2)
+    x2 = Conv2D(64,(1, 1),
+               padding='same',
+               use_bias=False,
+               strides=(1, 1))(x2)
+
+    x2 = keras.layers.normalization.BatchNormalization()(x2)
+    x2 = Activation('relu')(x2)
+    
+    x3 = Flatten()(x2)
+    x3 = Activation('relu')(x3)
+    x3 = Dropout(0.5)(x3)
+
+    # Steering channel
+    steer = Dense(output_dim)(x3)
+
+    # Collision channel
+    coll = Dense(output_dim)(x3)
+    coll = Activation('sigmoid')(coll)
+
+    # Define steering-collision model
+    model = Model(inputs=[img_input], outputs=[steer, coll])
+    print(model.summary())
+
+    return model
\ No newline at end of file
diff --git a/common_flags.py b/common_flags.py
old mode 100644
new mode 100755
index 49c818e..0310b82
--- a/common_flags.py
+++ b/common_flags.py
@@ -11,23 +11,23 @@ gflags.DEFINE_integer('img_height', 240, 'Target Image Height')
 gflags.DEFINE_integer('crop_img_width', 200, 'Cropped image widht')
 gflags.DEFINE_integer('crop_img_height', 200, 'Cropped image height')
 
-gflags.DEFINE_string('img_mode', "grayscale", 'Load mode for images, either '
+gflags.DEFINE_string('img_mode', "rgb", 'Load mode for images, either '
                      'rgb or grayscale')
 
 # Training
-gflags.DEFINE_integer('batch_size', 32, 'Batch size in training and evaluation')
-gflags.DEFINE_integer('epochs', 100, 'Number of epochs for training')
+gflags.DEFINE_integer('batch_size', 64, 'Batch size in training and evaluation')
+gflags.DEFINE_integer('epochs', 50, 'Number of epochs for training')
 gflags.DEFINE_integer('log_rate', 10, 'Logging rate for full model (epochs)')
 gflags.DEFINE_integer('initial_epoch', 0, 'Initial epoch to start training')
 
 # Files
-gflags.DEFINE_string('experiment_rootdir', "./model", 'Folder '
+gflags.DEFINE_string('experiment_rootdir', "./modelrun2", 'Folder '
                      ' containing all the logs, model weights and results')
-gflags.DEFINE_string('train_dir', "../training", 'Folder containing'
+gflags.DEFINE_string('train_dir', "../preprocessed/training", 'Folder containing'
                      ' training experiments')
-gflags.DEFINE_string('val_dir', "../validation", 'Folder containing'
+gflags.DEFINE_string('val_dir', "../preprocessed/validation", 'Folder containing'
                      ' validation experiments')
-gflags.DEFINE_string('test_dir', "../testing", 'Folder containing'
+gflags.DEFINE_string('test_dir', "../preprocessed/testing", 'Folder containing'
                      ' testing experiments')
 
 # Model
diff --git a/constants.py b/constants.py
old mode 100644
new mode 100755
diff --git a/data_preprocessing/time_stamp_matching.py b/data_preprocessing/time_stamp_matching.py
old mode 100644
new mode 100755
diff --git a/drone_control/dronet/README.md b/drone_control/dronet/README.md
old mode 100644
new mode 100755
diff --git a/drone_control/dronet/configs/outdoor.yaml b/drone_control/dronet/configs/outdoor.yaml
old mode 100644
new mode 100755
diff --git a/drone_control/dronet/dronet_control/CMakeLists.txt b/drone_control/dronet/dronet_control/CMakeLists.txt
old mode 100644
new mode 100755
diff --git a/drone_control/dronet/dronet_control/include/dronet_control/deep_navigation.h b/drone_control/dronet/dronet_control/include/dronet_control/deep_navigation.h
old mode 100644
new mode 100755
diff --git a/drone_control/dronet/dronet_control/launch/deep_navigation.launch b/drone_control/dronet/dronet_control/launch/deep_navigation.launch
old mode 100644
new mode 100755
diff --git a/drone_control/dronet/dronet_control/package.xml b/drone_control/dronet/dronet_control/package.xml
old mode 100644
new mode 100755
diff --git a/drone_control/dronet/dronet_control/src/deep_navigation.cpp b/drone_control/dronet/dronet_control/src/deep_navigation.cpp
old mode 100644
new mode 100755
diff --git a/drone_control/dronet/dronet_perception/CMakeLists.txt b/drone_control/dronet/dronet_perception/CMakeLists.txt
old mode 100644
new mode 100755
diff --git a/drone_control/dronet/dronet_perception/__init__.py b/drone_control/dronet/dronet_perception/__init__.py
old mode 100644
new mode 100755
diff --git a/drone_control/dronet/dronet_perception/launch/bebop_launch.launch b/drone_control/dronet/dronet_perception/launch/bebop_launch.launch
old mode 100644
new mode 100755
diff --git a/drone_control/dronet/dronet_perception/launch/dronet_launch.launch b/drone_control/dronet/dronet_perception/launch/dronet_launch.launch
old mode 100644
new mode 100755
diff --git a/drone_control/dronet/dronet_perception/launch/full_perception_launch.launch b/drone_control/dronet/dronet_perception/launch/full_perception_launch.launch
old mode 100644
new mode 100755
diff --git a/drone_control/dronet/dronet_perception/msg/CNN_out.msg b/drone_control/dronet/dronet_perception/msg/CNN_out.msg
old mode 100644
new mode 100755
diff --git a/drone_control/dronet/dronet_perception/package.xml b/drone_control/dronet/dronet_perception/package.xml
old mode 100644
new mode 100755
diff --git a/drone_control/dronet/dronet_perception/setup.py b/drone_control/dronet/dronet_perception/setup.py
old mode 100644
new mode 100755
diff --git a/drone_control/dronet/dronet_perception/src/Dronet/Dronet.py b/drone_control/dronet/dronet_perception/src/Dronet/Dronet.py
old mode 100644
new mode 100755
diff --git a/drone_control/dronet/dronet_perception/src/Dronet/__init__.py b/drone_control/dronet/dronet_perception/src/Dronet/__init__.py
old mode 100644
new mode 100755
diff --git a/drone_control/dronet/dronet_perception/src/Dronet/utils.py b/drone_control/dronet/dronet_perception/src/Dronet/utils.py
old mode 100644
new mode 100755
diff --git a/evaluation.py b/evaluation.py
old mode 100644
new mode 100755
diff --git a/images/architecture.png b/images/architecture.png
old mode 100644
new mode 100755
diff --git a/images/dataset.png b/images/dataset.png
old mode 100644
new mode 100755
diff --git a/img_utils.py b/img_utils.py
old mode 100644
new mode 100755
diff --git a/log_utils.py b/log_utils.py
old mode 100644
new mode 100755
diff --git a/logz.py b/logz.py
old mode 100644
new mode 100755
diff --git a/model/model_struct.json b/model/model_struct.json
deleted file mode 100644
index 1d5a9a5..0000000
--- a/model/model_struct.json
+++ /dev/null
@@ -1 +0,0 @@
-{"class_name": "Model", "keras_version": "2.0.2", "backend": "tensorflow", "config": {"input_layers": [["input_1", 0, 0]], "output_layers": [["dense_1", 0, 0], ["activation_8", 0, 0]], "name": "model_1", "layers": [{"class_name": "InputLayer", "name": "input_1", "config": {"batch_input_shape": [null, 200, 200, 1], "dtype": "float32", "name": "input_1", "sparse": false}, "inbound_nodes": []}, {"class_name": "Conv2D", "name": "conv2d_1", "config": {"bias_initializer": {"class_name": "Zeros", "config": {}}, "filters": 32, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"scale": 1.0, "distribution": "uniform", "mode": "fan_avg", "seed": null}}, "kernel_size": [5, 5], "activation": "linear", "activity_regularizer": null, "name": "conv2d_1", "bias_constraint": null, "kernel_constraint": null, "kernel_regularizer": null, "data_format": "channels_last", "bias_regularizer": null, "use_bias": true, "dilation_rate": [1, 1], "padding": "same", "trainable": true, "strides": [2, 2]}, "inbound_nodes": [[["input_1", 0, 0, {}]]]}, {"class_name": "MaxPooling2D", "name": "max_pooling2d_1", "config": {"trainable": true, "data_format": "channels_last", "strides": [2, 2], "padding": "valid", "name": "max_pooling2d_1", "pool_size": [3, 3]}, "inbound_nodes": [[["conv2d_1", 0, 0, {}]]]}, {"class_name": "BatchNormalization", "name": "batch_normalization_1", "config": {"epsilon": 0.001, "gamma_constraint": null, "momentum": 0.99, "beta_regularizer": null, "moving_mean_initializer": {"class_name": "Zeros", "config": {}}, "name": "batch_normalization_1", "center": true, "gamma_regularizer": null, "beta_initializer": {"class_name": "Zeros", "config": {}}, "axis": -1, "gamma_initializer": {"class_name": "Ones", "config": {}}, "scale": true, "moving_variance_initializer": {"class_name": "Ones", "config": {}}, "trainable": true, "beta_constraint": null}, "inbound_nodes": [[["max_pooling2d_1", 0, 0, {}]]]}, {"class_name": "Activation", "name": "activation_1", "config": {"activation": "relu", "trainable": true, "name": "activation_1"}, "inbound_nodes": [[["batch_normalization_1", 0, 0, {}]]]}, {"class_name": "Conv2D", "name": "conv2d_2", "config": {"bias_initializer": {"class_name": "Zeros", "config": {}}, "filters": 32, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"scale": 2.0, "distribution": "normal", "mode": "fan_in", "seed": null}}, "kernel_size": [3, 3], "activation": "linear", "activity_regularizer": null, "name": "conv2d_2", "bias_constraint": null, "kernel_constraint": null, "kernel_regularizer": {"class_name": "L1L2", "config": {"l2": 9.999999747378752e-05, "l1": 0.0}}, "data_format": "channels_last", "bias_regularizer": null, "use_bias": true, "dilation_rate": [1, 1], "padding": "same", "trainable": true, "strides": [2, 2]}, "inbound_nodes": [[["activation_1", 0, 0, {}]]]}, {"class_name": "BatchNormalization", "name": "batch_normalization_2", "config": {"epsilon": 0.001, "gamma_constraint": null, "momentum": 0.99, "beta_regularizer": null, "moving_mean_initializer": {"class_name": "Zeros", "config": {}}, "name": "batch_normalization_2", "center": true, "gamma_regularizer": null, "beta_initializer": {"class_name": "Zeros", "config": {}}, "axis": -1, "gamma_initializer": {"class_name": "Ones", "config": {}}, "scale": true, "moving_variance_initializer": {"class_name": "Ones", "config": {}}, "trainable": true, "beta_constraint": null}, "inbound_nodes": [[["conv2d_2", 0, 0, {}]]]}, {"class_name": "Activation", "name": "activation_2", "config": {"activation": "relu", "trainable": true, "name": "activation_2"}, "inbound_nodes": [[["batch_normalization_2", 0, 0, {}]]]}, {"class_name": "Conv2D", "name": "conv2d_4", "config": {"bias_initializer": {"class_name": "Zeros", "config": {}}, "filters": 32, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"scale": 1.0, "distribution": "uniform", "mode": "fan_avg", "seed": null}}, "kernel_size": [1, 1], "activation": "linear", "activity_regularizer": null, "name": "conv2d_4", "bias_constraint": null, "kernel_constraint": null, "kernel_regularizer": null, "data_format": "channels_last", "bias_regularizer": null, "use_bias": true, "dilation_rate": [1, 1], "padding": "same", "trainable": true, "strides": [2, 2]}, "inbound_nodes": [[["max_pooling2d_1", 0, 0, {}]]]}, {"class_name": "Conv2D", "name": "conv2d_3", "config": {"bias_initializer": {"class_name": "Zeros", "config": {}}, "filters": 32, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"scale": 2.0, "distribution": "normal", "mode": "fan_in", "seed": null}}, "kernel_size": [3, 3], "activation": "linear", "activity_regularizer": null, "name": "conv2d_3", "bias_constraint": null, "kernel_constraint": null, "kernel_regularizer": {"class_name": "L1L2", "config": {"l2": 9.999999747378752e-05, "l1": 0.0}}, "data_format": "channels_last", "bias_regularizer": null, "use_bias": true, "dilation_rate": [1, 1], "padding": "same", "trainable": true, "strides": [1, 1]}, "inbound_nodes": [[["activation_2", 0, 0, {}]]]}, {"class_name": "Add", "name": "add_1", "config": {"name": "add_1", "trainable": true}, "inbound_nodes": [[["conv2d_4", 0, 0, {}], ["conv2d_3", 0, 0, {}]]]}, {"class_name": "BatchNormalization", "name": "batch_normalization_3", "config": {"epsilon": 0.001, "gamma_constraint": null, "momentum": 0.99, "beta_regularizer": null, "moving_mean_initializer": {"class_name": "Zeros", "config": {}}, "name": "batch_normalization_3", "center": true, "gamma_regularizer": null, "beta_initializer": {"class_name": "Zeros", "config": {}}, "axis": -1, "gamma_initializer": {"class_name": "Ones", "config": {}}, "scale": true, "moving_variance_initializer": {"class_name": "Ones", "config": {}}, "trainable": true, "beta_constraint": null}, "inbound_nodes": [[["add_1", 0, 0, {}]]]}, {"class_name": "Activation", "name": "activation_3", "config": {"activation": "relu", "trainable": true, "name": "activation_3"}, "inbound_nodes": [[["batch_normalization_3", 0, 0, {}]]]}, {"class_name": "Conv2D", "name": "conv2d_5", "config": {"bias_initializer": {"class_name": "Zeros", "config": {}}, "filters": 64, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"scale": 2.0, "distribution": "normal", "mode": "fan_in", "seed": null}}, "kernel_size": [3, 3], "activation": "linear", "activity_regularizer": null, "name": "conv2d_5", "bias_constraint": null, "kernel_constraint": null, "kernel_regularizer": {"class_name": "L1L2", "config": {"l2": 9.999999747378752e-05, "l1": 0.0}}, "data_format": "channels_last", "bias_regularizer": null, "use_bias": true, "dilation_rate": [1, 1], "padding": "same", "trainable": true, "strides": [2, 2]}, "inbound_nodes": [[["activation_3", 0, 0, {}]]]}, {"class_name": "BatchNormalization", "name": "batch_normalization_4", "config": {"epsilon": 0.001, "gamma_constraint": null, "momentum": 0.99, "beta_regularizer": null, "moving_mean_initializer": {"class_name": "Zeros", "config": {}}, "name": "batch_normalization_4", "center": true, "gamma_regularizer": null, "beta_initializer": {"class_name": "Zeros", "config": {}}, "axis": -1, "gamma_initializer": {"class_name": "Ones", "config": {}}, "scale": true, "moving_variance_initializer": {"class_name": "Ones", "config": {}}, "trainable": true, "beta_constraint": null}, "inbound_nodes": [[["conv2d_5", 0, 0, {}]]]}, {"class_name": "Activation", "name": "activation_4", "config": {"activation": "relu", "trainable": true, "name": "activation_4"}, "inbound_nodes": [[["batch_normalization_4", 0, 0, {}]]]}, {"class_name": "Conv2D", "name": "conv2d_7", "config": {"bias_initializer": {"class_name": "Zeros", "config": {}}, "filters": 64, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"scale": 1.0, "distribution": "uniform", "mode": "fan_avg", "seed": null}}, "kernel_size": [1, 1], "activation": "linear", "activity_regularizer": null, "name": "conv2d_7", "bias_constraint": null, "kernel_constraint": null, "kernel_regularizer": null, "data_format": "channels_last", "bias_regularizer": null, "use_bias": true, "dilation_rate": [1, 1], "padding": "same", "trainable": true, "strides": [2, 2]}, "inbound_nodes": [[["add_1", 0, 0, {}]]]}, {"class_name": "Conv2D", "name": "conv2d_6", "config": {"bias_initializer": {"class_name": "Zeros", "config": {}}, "filters": 64, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"scale": 2.0, "distribution": "normal", "mode": "fan_in", "seed": null}}, "kernel_size": [3, 3], "activation": "linear", "activity_regularizer": null, "name": "conv2d_6", "bias_constraint": null, "kernel_constraint": null, "kernel_regularizer": {"class_name": "L1L2", "config": {"l2": 9.999999747378752e-05, "l1": 0.0}}, "data_format": "channels_last", "bias_regularizer": null, "use_bias": true, "dilation_rate": [1, 1], "padding": "same", "trainable": true, "strides": [1, 1]}, "inbound_nodes": [[["activation_4", 0, 0, {}]]]}, {"class_name": "Add", "name": "add_2", "config": {"name": "add_2", "trainable": true}, "inbound_nodes": [[["conv2d_7", 0, 0, {}], ["conv2d_6", 0, 0, {}]]]}, {"class_name": "BatchNormalization", "name": "batch_normalization_5", "config": {"epsilon": 0.001, "gamma_constraint": null, "momentum": 0.99, "beta_regularizer": null, "moving_mean_initializer": {"class_name": "Zeros", "config": {}}, "name": "batch_normalization_5", "center": true, "gamma_regularizer": null, "beta_initializer": {"class_name": "Zeros", "config": {}}, "axis": -1, "gamma_initializer": {"class_name": "Ones", "config": {}}, "scale": true, "moving_variance_initializer": {"class_name": "Ones", "config": {}}, "trainable": true, "beta_constraint": null}, "inbound_nodes": [[["add_2", 0, 0, {}]]]}, {"class_name": "Activation", "name": "activation_5", "config": {"activation": "relu", "trainable": true, "name": "activation_5"}, "inbound_nodes": [[["batch_normalization_5", 0, 0, {}]]]}, {"class_name": "Conv2D", "name": "conv2d_8", "config": {"bias_initializer": {"class_name": "Zeros", "config": {}}, "filters": 128, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"scale": 2.0, "distribution": "normal", "mode": "fan_in", "seed": null}}, "kernel_size": [3, 3], "activation": "linear", "activity_regularizer": null, "name": "conv2d_8", "bias_constraint": null, "kernel_constraint": null, "kernel_regularizer": {"class_name": "L1L2", "config": {"l2": 9.999999747378752e-05, "l1": 0.0}}, "data_format": "channels_last", "bias_regularizer": null, "use_bias": true, "dilation_rate": [1, 1], "padding": "same", "trainable": true, "strides": [2, 2]}, "inbound_nodes": [[["activation_5", 0, 0, {}]]]}, {"class_name": "BatchNormalization", "name": "batch_normalization_6", "config": {"epsilon": 0.001, "gamma_constraint": null, "momentum": 0.99, "beta_regularizer": null, "moving_mean_initializer": {"class_name": "Zeros", "config": {}}, "name": "batch_normalization_6", "center": true, "gamma_regularizer": null, "beta_initializer": {"class_name": "Zeros", "config": {}}, "axis": -1, "gamma_initializer": {"class_name": "Ones", "config": {}}, "scale": true, "moving_variance_initializer": {"class_name": "Ones", "config": {}}, "trainable": true, "beta_constraint": null}, "inbound_nodes": [[["conv2d_8", 0, 0, {}]]]}, {"class_name": "Activation", "name": "activation_6", "config": {"activation": "relu", "trainable": true, "name": "activation_6"}, "inbound_nodes": [[["batch_normalization_6", 0, 0, {}]]]}, {"class_name": "Conv2D", "name": "conv2d_10", "config": {"bias_initializer": {"class_name": "Zeros", "config": {}}, "filters": 128, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"scale": 1.0, "distribution": "uniform", "mode": "fan_avg", "seed": null}}, "kernel_size": [1, 1], "activation": "linear", "activity_regularizer": null, "name": "conv2d_10", "bias_constraint": null, "kernel_constraint": null, "kernel_regularizer": null, "data_format": "channels_last", "bias_regularizer": null, "use_bias": true, "dilation_rate": [1, 1], "padding": "same", "trainable": true, "strides": [2, 2]}, "inbound_nodes": [[["add_2", 0, 0, {}]]]}, {"class_name": "Conv2D", "name": "conv2d_9", "config": {"bias_initializer": {"class_name": "Zeros", "config": {}}, "filters": 128, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"scale": 2.0, "distribution": "normal", "mode": "fan_in", "seed": null}}, "kernel_size": [3, 3], "activation": "linear", "activity_regularizer": null, "name": "conv2d_9", "bias_constraint": null, "kernel_constraint": null, "kernel_regularizer": {"class_name": "L1L2", "config": {"l2": 9.999999747378752e-05, "l1": 0.0}}, "data_format": "channels_last", "bias_regularizer": null, "use_bias": true, "dilation_rate": [1, 1], "padding": "same", "trainable": true, "strides": [1, 1]}, "inbound_nodes": [[["activation_6", 0, 0, {}]]]}, {"class_name": "Add", "name": "add_3", "config": {"name": "add_3", "trainable": true}, "inbound_nodes": [[["conv2d_10", 0, 0, {}], ["conv2d_9", 0, 0, {}]]]}, {"class_name": "Flatten", "name": "flatten_1", "config": {"name": "flatten_1", "trainable": true}, "inbound_nodes": [[["add_3", 0, 0, {}]]]}, {"class_name": "Activation", "name": "activation_7", "config": {"activation": "relu", "trainable": true, "name": "activation_7"}, "inbound_nodes": [[["flatten_1", 0, 0, {}]]]}, {"class_name": "Dropout", "name": "dropout_1", "config": {"name": "dropout_1", "rate": 0.5, "trainable": true}, "inbound_nodes": [[["activation_7", 0, 0, {}]]]}, {"class_name": "Dense", "name": "dense_2", "config": {"bias_initializer": {"class_name": "Zeros", "config": {}}, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"scale": 1.0, "distribution": "uniform", "mode": "fan_avg", "seed": null}}, "activation": "linear", "activity_regularizer": null, "name": "dense_2", "units": 1, "kernel_constraint": null, "kernel_regularizer": null, "bias_regularizer": null, "use_bias": true, "trainable": true, "bias_constraint": null}, "inbound_nodes": [[["dropout_1", 0, 0, {}]]]}, {"class_name": "Dense", "name": "dense_1", "config": {"bias_initializer": {"class_name": "Zeros", "config": {}}, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"scale": 1.0, "distribution": "uniform", "mode": "fan_avg", "seed": null}}, "activation": "linear", "activity_regularizer": null, "name": "dense_1", "units": 1, "kernel_constraint": null, "kernel_regularizer": null, "bias_regularizer": null, "use_bias": true, "trainable": true, "bias_constraint": null}, "inbound_nodes": [[["dropout_1", 0, 0, {}]]]}, {"class_name": "Activation", "name": "activation_8", "config": {"activation": "sigmoid", "trainable": true, "name": "activation_8"}, "inbound_nodes": [[["dense_2", 0, 0, {}]]]}]}}
\ No newline at end of file
diff --git a/model/model_weights.h5 b/model/model_weights.h5
deleted file mode 100644
index 2b55306..0000000
Binary files a/model/model_weights.h5 and /dev/null differ
diff --git a/plot_loss.py b/plot_loss.py
old mode 100644
new mode 100755
diff --git a/plot_results.py b/plot_results.py
old mode 100644
new mode 100755
diff --git a/utils.py b/utils.py
old mode 100644
new mode 100755
index 36840b5..5e62e34
--- a/utils.py
+++ b/utils.py
@@ -14,6 +14,7 @@ import img_utils
 
 
 class DroneDataGenerator(ImageDataGenerator):
+    #print('Entered Drone Data Generator')
     """
     Generate minibatches of images and labels with real-time augmentation.
 
@@ -106,6 +107,7 @@ class DroneDirectoryIterator(Iterator):
 
         # Conversion of list into array
         self.ground_truth = np.array(self.ground_truth, dtype = K.floatx())
+        #print('Ground Truth', self.ground_truth)
 
         assert self.samples > 0, "Did not find any data"
 
@@ -159,6 +161,58 @@ class DroneDirectoryIterator(Iterator):
                     self.ground_truth.append(ground_truth[frame_number])
                     self.exp_type.append(exp_type)
                     self.samples += 1
+        #print('HEREEEEEEEEEEEE')
+
+    #Adding this function and calling it in next
+    def _get_batches_of_transformed_samples(self, index_array):
+        """
+        Public function to fetch next batch.
+        
+        # Returns
+            The next batch of images and labels.
+        """
+        #print('index array transformed:',index_array)
+        current_batch_size = index_array.shape[0]
+        #current_batch_size = index_array[0].shape[0]
+
+        # Image transformation is not under thread lock, so it can be done in
+        # parallel
+        batch_x = np.zeros((current_batch_size,) + self.image_shape,
+                dtype=K.floatx())
+        batch_steer = np.zeros((current_batch_size, 2,),
+                dtype=K.floatx())
+        batch_coll = np.zeros((current_batch_size, 2,),
+                dtype=K.floatx())
+
+        grayscale = self.color_mode == 'grayscale'
+
+        # Build batch of image data
+        for i, j in enumerate(index_array):
+            fname = self.filenames[j]
+            x = img_utils.load_img(os.path.join(self.directory, fname),
+                    grayscale=grayscale,
+                    crop_size=self.crop_size,
+                    target_size=self.target_size)
+
+            x = self.image_data_generator.random_transform(x)
+            x = self.image_data_generator.standardize(x)
+            batch_x[i] = x
+
+            # Build batch of steering and collision data
+            if self.exp_type[index_array[i]] == 1:
+                # Steering experiment (t=1)
+                batch_steer[i,0] =1.0
+                batch_steer[i,1] = self.ground_truth[index_array[i]]
+                batch_coll[i] = np.array([1.0, 0.0])
+            else:
+                # Collision experiment (t=0)
+                batch_steer[i] = np.array([0.0, 0.0])
+                batch_coll[i,0] = 0.0
+                batch_coll[i,1] = self.ground_truth[index_array[i]]
+
+        batch_y = [batch_steer, batch_coll]
+        return batch_x, batch_y
+
 
     def next(self):
         """
@@ -169,8 +223,13 @@ class DroneDirectoryIterator(Iterator):
         """
         with self.lock:
             index_array = next(self.index_generator)
-            current_batch_size = index_array.shape[0]
+            #print('index array in utils',index_array)
+            #current_batch_size = index_array.shape[0]
+            #current_batch_size = index_array[0].shape[0]
 
+        return self._get_batches_of_transformed_samples(index_array)
+
+        ''''
         # Image transformation is not under thread lock, so it can be done in
         # parallel
         batch_x = np.zeros((current_batch_size,) + self.image_shape,
@@ -183,7 +242,7 @@ class DroneDirectoryIterator(Iterator):
         grayscale = self.color_mode == 'grayscale'
 
         # Build batch of image data
-        for i, j in enumerate(index_array):
+        for i, j in enumerate(index_array[0]):
             fname = self.filenames[j]
             x = img_utils.load_img(os.path.join(self.directory, fname),
                     grayscale=grayscale,
@@ -208,7 +267,7 @@ class DroneDirectoryIterator(Iterator):
 
         batch_y = [batch_steer, batch_coll]
         return batch_x, batch_y
-
+        '''
 
 
 def compute_predictions_and_gt(model, generator, steps,
